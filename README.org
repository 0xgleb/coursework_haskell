* Introduction

  The purpose of this project is to compare to popular languages in different programming paradigms: Haskell vs Python. Haskell is one of the most popular functional programming languages and one of the most "functional" in the sense that it doesn't support imperative programming or OOP. If you type into Google "Haskell vs " the first thing that comes up is "Haskell vs Python" which is one of the reasons I decided to compare Haskell with Python. Also Python is very different to Haskell, so we can compare lots of different aspects and different ideas.

  Here are some differences between Haskell and Python:

  |---------------------+------------------------------+----------------------------------------------------------------------|
  |                     | Haskell                      | Python                                                               |
  |---------------------+------------------------------+----------------------------------------------------------------------|
  | Paradigms           | Functional                   | OOP, imperative, procedural. Has elements of functional programming. |
  | Evaluation strategy | Lazy/non-strict              | Strict                                                               |
  | Type system         | Static, strong inferred      | Dynamic, strong, duck.                                               |
  | Execution (default) | Compiled or intepreted (GHC) | Interpreted (CPython)                                                |
  |---------------------+------------------------------+----------------------------------------------------------------------|

  Python is one year younger than Haskell and some of it's features were influenced by Haskell.
  
* Basics of Haskell

  Most programmers know Python to some extend, however very few people know Haskell. This is why I'm going to start this comparison by introducing basics of Haskell.

** Program structure

   The program structure in Haskell is similar to the program structure of C. We have the main "function" that runs other "functions" and that's what makes everything work. The reason why I wrote function in quotes is because these are not Haskell functions, these are IO actions (similar to procedures).

** Hello World +

   #+BEGIN_SRC haskell
   main :: IO ()
   main = print "Hello World!"
   #+END_SRC

   Now let's write a console program that asks the user for their name and then prints "Hello <username>!"

   Haskell's IO parts of the program can look very similar to "normal" languages.

   #+BEGIN_SRC haskell
   main :: IO ()
   main = do
       print "What's your name?"
       name <- getLine
       print ("Hello " ++ name ++ "!")
   #+END_SRC

   But we can write this in functional style and it would look like this:

   #+BEGIN_SRC haskell
   main :: IO ()
   main = print "What's your name?" >> getLine >>= print . (++ "!") . ("Hello " ++)
   #+END_SRC

   This does exactly the same thing. First it prints "What's your name", then it reads user's input and then redirects it to ~print . (++ "!") . ("Hello " ++)~. This is a composition prints the result after adding the lines. This is a more compact way of writing that and it doesn't use the ~name~ variable.

** GHCi

   GHC (Glasgow Haskell Compiler) is the standard Haskell compiler. GHCi is a command line interactive environment where you can execute Haskell code without writing it to a file.

   Let's start by looking at simple expressions in GHCi.

*** Numbers

   #+BEGIN_SRC haskell
   Prelude> 10
   10
   Prelude> 10 + 61
   71
   Prelude> (10 + 61) ^ 2
   5041
   Prelude> (10 + 61) ** 2
   5041.0
   Prelude> (10 - 61) / 20
   -2.55
   #+END_SRC

   Here are some basic arithmetic operations. The only ones that are worth explaining are ~(^)~ and ~(**)~. The difference between these 2 is that ~(^)~ only works with non-negative integer powers. And ~(**)~ requires both arguments to be floating.

*** Lists

    #+BEGIN_SRC haskell
    Prelude> [1, 2, 3, 4, 5]
    [1,2,3,4,5]
    Prelude> [1..5]
    [1,2,3,4,5]
    Prelude> 0 : [1..5]
    [0,1,2,3,4,5]
    Prelude> 1 : []
    [1]
    #+END_SRC

    ~(:)~ takes a value and a list of values of the same type and returns the list with the first argument in the beginning of the list.

    In Haskell all elements of a list must have the same type.

    #+BEGIN_SRC haskell
    Prelude> ['?', 1]
    
    <interactive>:21:7: error:
        * No instance for (Num Char) arising from the literal `1'
        * In the expression: 1
          In the expression: ['?', 1]
          In an equation for `it': it = ['?', 1]
    #+END_SRC

*** Characters and strings

    In Haskell strings are just lists of characters. For characters we use single quotes and for strings double quotes.

    #+BEGIN_SRC haskell
    Prelude> ['H', 'e', 'l', 'l', 'o']
    "Hello"
    Prelude> "Hello"
    "Hello"
    Prelude> "test" == ['t', 'e', 's', 't']
    True
    #+END_SRC

** Variables and functions
** Types
** Type classes
** IO

* Procedural, object orientied, and functional programming.
** Imperative and declarative programming
   *Imperative programming* is a programming paradigm that uses statements that change a program's state.

   *Declarative programming* is a programming paradigm that expresses the logic of a computation without describing the order of instructions. It tries to describe what needs to be accomplished instead of describing how to accomplish it.

   So the difference between imperative programming and declarative programming is that declarative programming doesn't specify the order of instructions.
** Expressiveness
   
   Let's take a look at several code examples in Python and Haskell.

*** Hello World

    Python:

    #+BEGIN_SRC python
    print "Hello World"
    #+END_SRC

    Haskell:

    #+BEGIN_SRC haskell
    main :: IO () -- type declaration is not necessary, if I didn't specify it then the compiler would derive it
    main = print "Hello World"
    #+END_SRC

*** A program that reads a name and print "Hello <username>"

    Python:

    #+BEGIN_SRC python

    #+END_SRC

    Haskell (procedural IO style):

    #+BEGIN_SRC haskell
    main :: IO ()
    main = do name <- getLine
              print ("Hello " ++ name)
    #+END_SRC

    Haskell (functional style):

    #+BEGIN_SRC haskell
    main :: IO ()
    main = getLine >>= print . ("Hello " ++)
    #+END_SRC

** Polymorphism

   Both Haskell and Python have polymorphism, however there are sertain differences. Haskell doesn't support OOP, so it has a different types of polymorphisms: parametric polymorphism and ad hoc polymorhpism. For parametric polymorphism Haskell uses type variables.

   For example, let's take a look at identity function.

   #+BEGIN_SRC haskell
   id :: a -> a
   id x = x
   #+END_SRC

   ~a~ can be replaced with any type. If we want to use a type that has specific function implemented for it then we use type classes. A type class defines a list of variables and/or functions. A type implements the type class if it implements all the functions.

   #+BEGIN_SRC haskell
   class Show a where
       show :: a -> String

   data Example = Example

   instance Show Example where
       show Example = "Example"
   #+END_SRC

   Now we can use this with parametric polymorhpism.

   #+BEGIN_SRC haskell
   showAndReverse :: Show a => a -> String
   showAndReverse = reverse . show
   #+END_SRC

* Haskell vs functional elements of Python
* Strict and lazy/non-strict evaluation

  Haskell has a very interesting execution scheme. It doesn't execute expressions until it needs the result. It can make our code simpler and more modular, but it can also be confusing wheen it comes to estimating performance and memory usage. For example this simple expression that sums all numbers from 1 to 10^8 ~foldl 0 [1..10^8]~ requires gigabytes of memory to evaluate. But if we import the strict version of this funciton ~foldl'~ from the ~Data.List~ module, everything's ok.

** How lazy evaluation in Haskell works?

*** Graph reduction

    Haskell programs are executed by evaluating expressions. The primary idea is function application. Here is a simple function:

    #+BEGIN_SRC haskell
    square x = x*x
    #+END_SRC

    Let's see how the following expression gets evaluated:

    #+BEGIN_SRC haskell
    square (1+2)
    => (1+2)*(1+2) -- replacing the left hand side
    => 3*(1+2)
    => 3*3
    => 9
    #+END_SRC

    We calculated ~(1+2)~ twice, to avoid that we use graph reduction method. In this graph every block is a function application. Our situation can be represented by the following graph:

    [[https://hackhands.com/data/blogs/ClosedSource/lazy-evaluation-works-haskell/assets/blocks-square-0.png]]

    This representation is similar to the way the compiler actually represents expressions with pointers. When a programmer defines a function they define a reduction rule, then when the function is applied the graph gets reduced until it becomes a basic expression. Any expression can be represented using graphs.

    Our function corresponds with this rule:

    [[https://hackhands.com/data/blogs/ClosedSource/lazy-evaluation-works-haskell/assets/blocks-square-rule.png]]
    
    ~x~ is a placeholder for a subgraph. And when arguments get duplicated they point to the same subgraph, hence identical graphs don't get reduced multiple times.

    Any subgraph that follows the rules is called a reducible expression or redex. In our case with have two redexes: function ~square~ and addition ~+~. If we start with ~square~ then we'll get this:

    [[https://habrastorage.org/getpro/habr/post_images/295/429/ede/295429ede71982a0ce68544095ffed35.png]]

    At every step the highlighted rectangle gets updated.

*** Normal form

    If the graph is not a redex then it means that we already reduced everything and got the result that we wanted. In the last example the normal form was a number, but constructors of algebraic data types like ~Just~, ~Nothing~, or lists constructors ~:~ and ~[]~ are not reducible. Even though these are functions they can't be reduced, that's because they were defined using ~data~ and don't have a right-hand side. For example, graph:

    [[https://habrastorage.org/getpro/habr/post_images/bd7/1ca/4f6/bd71ca4f639ea360db4b9966446e5459.png]]

    By definition a normal graph needs to be finite and it shouldn't have cycles. Infinite recursion is not normal.

    #+BEGIN_SRC haskell
    ones = 1 : ones
    #+END_SRC

    Corresponds to the following cyclic graph.

    [[https://habrastorage.org/getpro/habr/post_images/76b/740/316/76b740316cb9f87f024dbe341cd65acc.png]]

    It's not a redex and also not in the normal form - the tail of the list points to the list itself, making an infinite recursion.

    In Haskell expressions usually don't get to the normal form. Quite often we stop when we get to the weak head normal form (WHNF). If a graph is in WHNF then it's top node is a constructor. Like expression ~(7+12):[]~ or graph

    [[https://habrastorage.org/getpro/habr/post_images/1ec/bb9/b87/1ecbb9b873d806a42ef7e5e42aa49a16.png]]

    is in WHNF, it's top node is a constructor of a list ~(:)~. And it's not the normal form because the first argument is a redex.

    List ~ones~ is also in WHNF, it's top node is a constructor. In Haskell we can create and use infinite lists! They work just fine.

** Execution order, lazy evaluation

   Often expressions have multiple redexes. Does the order at which we reduce them matter?

   Most languages use the strategy that reduces arguments to the normal form before reducing the function. However, most Haskell compilers use a different evaluation order called *lazy*. It first reduces the top function application. That may require calculating some of the arguments, but only as many as it needs. Let's take a look at this expression with pattern matching. The arguments will get evaluated from left to right until the top node contains a constructor. If pattern matching isn't used then the arguments don't get evaluated. If you pattern match a constructor then the argument gets reduced to WHNF.

   For example:

   #+BEGIN_SRC haskell
   (&&) :: Bool -> Bool -> Bool
   True  && x = x
   False && x = False
   #+END_SRC

   This defines two reduction rules:

   [[https://habrastorage.org/getpro/habr/post_images/dc4/eed/151/dc4eed15184fe1bc3325378d5c7a1706.png]]

   [[https://habrastorage.org/getpro/habr/post_images/dc4/eed/151/dc4eed15184fe1bc3325378d5c7a1706.png]]

   Now let's take a look at this expression:

   #+BEGIN_SRC haskell
   ('H' == 'i') && ('a' == 'm')
   #+END_SRC

   Both of the arguments are redexes. Cause of pattern matching the first argument will get evaluated. Then the graph will get reduced without reducing the second argument.
   
** Performance

   It's not hard to prove that for lazy evaluation we would need fewer number of calculations or the same as for eager evaluation. Also it can compute expressions with errors in it, such as

   #+BEGIN_SRC haskell
   a = 1
   b = 2
   (a == b) && (1 == (b/0))
   #+END_SRC

   The second argument of ~(&&)~ will never get evaluated, hence the second argument of the second ~(==)~ will never get evaluated, thus we'll never divide by zero and get an exception.

   However, if we look at the memory usage we can get some problems. Sometimes an expression reduced to normal form can use more memory than a redex, and it's vice versa. Let's take a look at examples of both cases.

   #+BEGIN_SRC haskell
   enumFromTo 1 1000
   #+END_SRC

   This expression generates a list with numbers from 1 to 1000. The list itself takes much more space than the expression.

   On the other hand we have situations where a simple expression would use ~foldl 0 [1..10^8]~ gigabytes of RAM to evaluate lazily.
   
   Here is another example:

   #+BEGIN_SRC haskell
   ((((0 + 1) + 2) + 3) + 4)
   #+END_SRC

   The graph that represents this expression takes more space than the normal form of the expression - ~10~.

   However Haskell allows you to force reduction using the ~seq~ combinator.

   #+BEGIN_SRC haskell
   seq :: a -> b -> b
   #+END_SRC

   If you look at the type signature you may think that it's exactly the same as the ~const~ function, however they are not the same. ~seq~ reduces the first argument to the WHNF and then returns the second argument. ~const~ doesn't do anything with the first argument. It's important to remember that ~seq~ doesn't reduce the first argument to the normal form. For example, if we are reading a list of lines ~l~ from a file, we can't just use ~seq l ...~ to force Haskell to finish reading the list. This would just force it to reading the first line, because that's enough to know the constructor. To force Haskell to finish reading the file we need to use ~seq (length l) ...~. The only way to reduce ~length l~ to the weak head normal form is to find the length, hence to read the entire file. But in other cases this might not work, for example ~length $ (+7) <$> [1..10]~ will find the length without adding any numbers.

   Here is a standard use case of ~seq~ that every Haskell programmer should know - strict left fold. Here is how ~foldl~ is defined in Prelude (Haskell's standard library):

   #+BEGIN_SRC haskell
   foldl :: (a -> b -> a) -> a -> [b] -> a
   foldl f a []     = a
   foldl f a (x:xs) = foldl f (f a x) xs
   #+END_SRC

   Say we want to sum all numbers from 1 to 100 (~[1..100]~). For that we would use the expression ~foldl (+) 0 [1..100]~. Here is how the evaluation process would look in that case:

   #+BEGIN_SRC haskell
   foldl (+) 0 [1..100]
   => foldl (+) 0 (1:[2..100])
   => foldl (+) (0 + 1) [2..100]
   => foldl (+) (0 + 1) (2:[3..100])
   => foldl (+) ((0 + 1) + 2) [3..100]
   ...
   #+END_SRC

* Algebraic data types vs classes
* Dynamic types vs static types
* Non-pure functions in Haskell and Python
* Debugging
* Fields vs lens
* Concurrency
* Use cases
